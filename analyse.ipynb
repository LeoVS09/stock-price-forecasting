{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly standard imports\n",
    "import plotly.graph_objs as go\n",
    "import chart_studio.plotly as py\n",
    "\n",
    "# Cufflinks wrapper on plotly\n",
    "import cufflinks\n",
    "\n",
    "# Data science imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Options for pandas\n",
    "pd.options.display.max_columns = 30\n",
    "\n",
    "# Display all cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "cufflinks.go_offline(connected=True)\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Set global theme\n",
    "cufflinks.set_config_file(world_readable=True, theme='pearl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore input dataset\n",
    "\n",
    "Will use target dataset [Bitcoin Historical Data](https://www.kaggle.com/mczielinski/bitcoin-historical-data)\n",
    "\n",
    "Bitcoin data at 1-min intervals from select exchanges, Jan 2012 to Dec 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.load_datasets import load_input_dataset\n",
    "\n",
    "input_dataset = load_input_dataset()\n",
    "\n",
    "input_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will explore full input dataset, some values contain NaN, which not ineraptebale by sweetviz, so will use timestamp as target feature for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz as sv\n",
    "\n",
    "analyse_report = sv.analyze([input_dataset, 'Input'], target_feat=\"Timestamp\")\n",
    "analyse_report.show_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will take one timestamp per hour for faster interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_dataset = input_dataset[59::60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "timestamp need interprate as date for charts processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_timestamps = hours_dataset.pop('Timestamp')\n",
    "hours_datetime = pd.to_datetime(raw_timestamps, unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature evalution over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hours_features = hours_dataset[['Open', 'Close', 'Weighted_Price', 'Volume_(BTC)', 'Volume_(Currency)']]\n",
    "hours_features.index = hours_datetime\n",
    "\n",
    "hours_features.iplot(\n",
    "    subplots=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will take only last three yers, because they have data without missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 24\n",
    "year = (365)*day\n",
    "\n",
    "years_count = 3.5\n",
    "items_count = round(years_count * year)\n",
    "\n",
    "last_years_dataset = hours_dataset[-1 * items_count:]\n",
    "last_years_datetime = hours_datetime[-1 * items_count:]\n",
    "\n",
    "last_years_dataset.head()\n",
    "len(last_years_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_years_features = last_years_dataset[['Open', 'Close', 'Weighted_Price', 'Volume_(BTC)', 'Volume_(Currency)']]\n",
    "last_years_features.index = last_years_datetime\n",
    "\n",
    "last_years_features.iplot(\n",
    "    subplots=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is have NaN\n",
    "last_years_dataset.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_years_dataset.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_years_dataset_with_time = input_dataset[59::60][-1 * items_count:]\n",
    "len(last_years_dataset_with_time)\n",
    "len(last_years_dataset)\n",
    "\n",
    "nan_datafreame = last_years_dataset_with_time[last_years_dataset_with_time.isna().any(axis=1)]\n",
    "nan_datafreame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_hours_datetime = pd.to_datetime(nan_datafreame.pop('Timestamp'), unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_features =  nan_datafreame['Weighted_Price']\n",
    "nan_features.index = nan_hours_datetime\n",
    "nan_features.iplot(\n",
    "    kind='scatter',\n",
    "    mode='markers'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check depenence of trading and price from date in year and time of day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly define function for display frequiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_log_freaquency(series):\n",
    "    fft = tf.signal.rfft(series)    \n",
    "    f_per_dataset = np.arange(0, len(fft))\n",
    "\n",
    "    n_samples_h = len(series)\n",
    "    hours_per_year = 24*365.2524\n",
    "    years_per_dataset = n_samples_h/(hours_per_year)\n",
    "\n",
    "    f_per_year = f_per_dataset/years_per_dataset\n",
    "    plt.step(f_per_year, np.abs(fft))\n",
    "    plt.xscale('log')\n",
    "    plt.xticks([1, 365.2524], labels=['1/Year', '1/day'])\n",
    "    _ = plt.xlabel('Frequency (log scale)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency of price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_nan = last_years_dataset.dropna()\n",
    "\n",
    "plot_log_freaquency(without_nan['Weighted_Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency of transaction volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_freaquency(without_nan['Volume_(Currency)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(tfds.as_numpy(train_data), columns=['text', 'type'])\n",
    "\n",
    "train_df['type'] = train_df['type'].apply(humanize_label)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Training dataset records', len(train_df.index))\n",
    "\n",
    "train_df['type'].iplot(\n",
    "    kind='hist',\n",
    "    yTitle='count',\n",
    "    xTitle='Type',\n",
    "    title='Training data distribution'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(tfds.as_numpy(test_data), columns=['text', 'type'])\n",
    "\n",
    "test_df['type'] = test_df['type'].apply(humanize_label)\n",
    "\n",
    "test_df[30:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing dataset records', len(test_df.index))\n",
    "\n",
    "neutralSeries = test_df.apply(lambda x: True if x['type'] == 'neutral' else False, axis=1)\n",
    "print('Count of neutral rows', len(neutralSeries[neutralSeries == True].index))\n",
    "\n",
    "test_df['type'].iplot(\n",
    "    kind='hist',\n",
    "    yTitle='count',\n",
    "    xTitle='Type',\n",
    "    title='Testing data distribution'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check preprocessed training datasets distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prep_df = pd.DataFrame(tfds.as_numpy(train_prep_dataset), columns=['text', 'type'])\n",
    "\n",
    "train_prep_df['type'] = train_prep_df['type'].apply(humanize_label)\n",
    "\n",
    "train_prep_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Training dataset records', len(train_prep_df.index))\n",
    "\n",
    "train_prep_df['type'].iplot(\n",
    "    kind='hist',\n",
    "    yTitle='count',\n",
    "    xTitle='Type',\n",
    "    title='Preprocessed training data distribution'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prep_df = pd.DataFrame(tfds.as_numpy(test_prep_dataset), columns=['text', 'type'])\n",
    "\n",
    "test_prep_df['type'] = test_prep_df['type'].apply(humanize_label)\n",
    "\n",
    "test_prep_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training dataset records', len(test_prep_df.index))\n",
    "\n",
    "test_prep_df['type'].iplot(\n",
    "    kind='hist',\n",
    "    yTitle='count',\n",
    "    xTitle='Type',\n",
    "    title='Preprocessed testing data distribution'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./metrics/training.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['epoch', 'accuracy', 'val_accuracy']].iplot(\n",
    "    x='epoch',\n",
    "    mode='lines+markers',\n",
    "    xTitle='epoch',\n",
    "    yTitle='accuracy', \n",
    "    title='Training accuracy',\n",
    "    linecolor='black',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['epoch', 'loss', 'val_loss']].iplot(\n",
    "    x='epoch',\n",
    "    mode='lines+markers',\n",
    "    xTitle='epoch',\n",
    "    yTitle='accuracy', \n",
    "    title='Losses'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "### Load probability model\n",
    "\n",
    "which can give predictions on model classes\n",
    "\n",
    "0 - bad review, 1 - good revie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.predict import get_probability_model\n",
    "\n",
    "model = get_probability_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Firstly will try predict on some data from training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.predict import get_text_and_label_from_dataset, predict\n",
    "REVIEW_INDEX = 110\n",
    "\n",
    "text, real_label = get_text_and_label_from_dataset(REVIEW_INDEX)\n",
    "\n",
    "print('text for prediction\\n\\n', text, '\\n')\n",
    "\n",
    "predicted_label, predictions = predict(text, model)\n",
    "\n",
    "print(label_categories[predicted_label], 'review')\n",
    "\n",
    "print('\\n\\nPredicted label:', predicted_label, 'real label: ', real_label, 'predictions:', predictions)\n",
    "if (predicted_label == real_label):\n",
    "    print('Successfully predicted')\n",
    "else:\n",
    "    print('Failed to predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then will try predict hadnwritten text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can change text and check model\n",
    "hadwriten = 'This is good film'\n",
    "\n",
    "print('Hendwriten text:\\n', hadwriten, '\\n')\n",
    "\n",
    "handwriten_label, predictions = predict(hadwriten, model)\n",
    "\n",
    "print(label_categories[predicted_label], 'review')\n",
    "\n",
    "print('Probabilities', predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
